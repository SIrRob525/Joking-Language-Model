{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop, SGD\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will read the data from the file and drop NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "      <th>body</th>\n",
       "      <th>punchline</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>Did you hear about the guy from Prague wearing...</td>\n",
       "      <td>The Czech's in the mail.</td>\n",
       "      <td>Did you hear about the guy from Prague wearing...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.361149e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77428</th>\n",
       "      <td>Cannibal Someone who is fed up with people</td>\n",
       "      <td>Someone who is fed up with people</td>\n",
       "      <td>Cannibal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.424818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316192</th>\n",
       "      <td>I went to hell and I was playing video games S...</td>\n",
       "      <td>Sucked since it was just never ending loading ...</td>\n",
       "      <td>I went to hell and I was playing video games</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504932e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84211</th>\n",
       "      <td>Blowjobs did not live up to my expectations. T...</td>\n",
       "      <td>They suck.</td>\n",
       "      <td>Blowjobs did not live up to my expectations.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.427739e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264880</th>\n",
       "      <td>There's a 86.7 percent chance of a Zombie outb...</td>\n",
       "      <td>Hopefully the first zombie is black.</td>\n",
       "      <td>There's a 86.7 percent chance of a Zombie outb...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.489582e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     joke  \\\n",
       "20508   Did you hear about the guy from Prague wearing...   \n",
       "77428          Cannibal Someone who is fed up with people   \n",
       "316192  I went to hell and I was playing video games S...   \n",
       "84211   Blowjobs did not live up to my expectations. T...   \n",
       "264880  There's a 86.7 percent chance of a Zombie outb...   \n",
       "\n",
       "                                                     body  \\\n",
       "20508                            The Czech's in the mail.   \n",
       "77428                   Someone who is fed up with people   \n",
       "316192  Sucked since it was just never ending loading ...   \n",
       "84211                                          They suck.   \n",
       "264880               Hopefully the first zombie is black.   \n",
       "\n",
       "                                                punchline  score          date  \n",
       "20508   Did you hear about the guy from Prague wearing...   64.0  1.361149e+09  \n",
       "77428                                            Cannibal    0.0  1.424818e+09  \n",
       "316192       I went to hell and I was playing video games    0.0  1.504932e+09  \n",
       "84211        Blowjobs did not live up to my expectations.    5.0  1.427739e+09  \n",
       "264880  There's a 86.7 percent chance of a Zombie outb...    2.0  1.489582e+09  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"rJokesData/data/preprocessed.csv\")\n",
    "df = df.dropna(axis=0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_context` will be out parameter which tells the model how many words to check to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_context = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the text into tokens. We will separate the jokes using `#` symbols and see if the model will predict it as the end of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154768303\n",
      "I hope you're all getting your Walter Cronkite jokes in order. He's next.\n",
      "Here's mine.   Ed McMahon, David Carradine, Farrah Fawcett, Michael Jackson,\n",
      "Billy Mays, and Walter Cronkite walk into a bar.   And die.   Your turn.  #  #\n",
      "#  #  #  #  #  #  #  # What is the only thing a woman can say that w\n"
     ]
    }
   ],
   "source": [
    "text = (\" # \" * n_context).join(list(df.loc[:, \"joke\"]))\n",
    "print(len(text))\n",
    "print(textwrap.fill(text[:300], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david', 'carradine', 'farrah', 'fawcett', 'michael', 'jackson', 'billy', 'mays', 'and', 'walter', 'cronkite', 'walk', 'into', 'a', 'bar', 'and', 'die', 'your', 'turn', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', 'what']\n",
      "32093292\n",
      "134157\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|#')\n",
    "tokens = tokenizer.tokenize(text.lower())\n",
    "print(tokens[20:50])\n",
    "print(len(tokens))\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that we have over 32 million words in total and over 130 000 unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us limit the number of tokens so that the model learns faster and the kernel does not crash. This will negatively affect the quality though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[:80000] #NOTE: for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7501\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set(tokens)\n",
    "tokens_dict = {word: i for i, word in enumerate(unique_tokens)}\n",
    "reverse_tokens_dict = {i: word for i, word in enumerate(unique_tokens)}\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: bag_of_words + PCA + LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create `X` and `y` datasets using the bag-of-words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79990/79990 [00:00<00:00, 524867.89it/s]\n"
     ]
    }
   ],
   "source": [
    "input_words = []\n",
    "next_words = []\n",
    "for i in tqdm(range(len(tokens) - n_context)):\n",
    "    input_words.append(tokens[i:i+n_context])\n",
    "    next_words.append(tokens[i+n_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79990/79990 [00:00<00:00, 83837.66it/s]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros(shape=(len(input_words), n_context, len(tokens_dict)), dtype=bool)\n",
    "y = np.zeros(shape=(len(input_words), len(tokens_dict)), dtype=bool)\n",
    "\n",
    "for i, words in enumerate(tqdm(input_words)):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i, j, tokens_dict[words[j]]] = 1\n",
    "    y[i, tokens_dict[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the number of PCA components. Also we will perform Incremental PCA as out dataset is quite large, so we also set the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_pca = 100\n",
    "pca_batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/80 [02:31<3:20:03, 151.95s/it]"
     ]
    }
   ],
   "source": [
    "pca = IncrementalPCA(n_components=n_components_pca)\n",
    "X = X.reshape(-1, X.shape[-1])\n",
    "\n",
    "#fit X\n",
    "for i in tqdm(range(X.shape[0] // pca_batch_size + 1)):\n",
    "    chunk = X[i * pca_batch_size : (i + 1) * pca_batch_size]\n",
    "    pca.partial_fit(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#transform X\n",
    "X_transformed = None\n",
    "for i in tqdm(range(X.shape[0] // pca_batch_size + 1)):\n",
    "    chunk = X[i * pca_batch_size : (i + 1) * pca_batch_size]\n",
    "    chunk = pca.transform(chunk)\n",
    "    if X_transformed is None:\n",
    "        X_transformed = chunk\n",
    "    else:\n",
    "        X_transformed = np.vstack((X_transformed, chunk))\n",
    "        \n",
    "X = X_transformed\n",
    "X = X.reshape(-1, n_context, X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9990, 10, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now out transformed dataset has less features and out model will train faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use simple model: two LSTM layers + linear + activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(n_context, n_components_pca), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(unique_tokens)))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/fadeevaaleksandra/Desktop/Joking-Language-Model/language_model.ipynb Ячейка 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fadeevaaleksandra/Desktop/Joking-Language-Model/language_model.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m RMSprop(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fadeevaaleksandra/Desktop/Joking-Language-Model/language_model.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39moptimizer, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fadeevaaleksandra/Desktop/Joking-Language-Model/language_model.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X, y, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mhistory\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(X, y, batch_size=100, epochs=50, shuffle=True).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trains quite fast and the quality is great too. Now let us generate some text and see if it is comprehensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, n_best):\n",
    "    input_text = input_text.lower()\n",
    "    X = np.zeros((1, n_context, len(unique_tokens)))\n",
    "    for i, word in enumerate(input_text.split()):\n",
    "        X[0, i, tokens_dict[word]] = 1\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    X = pca.transform(X)\n",
    "    X = X.reshape(-1, n_context, X.shape[-1])\n",
    "        \n",
    "    predictions = model.predict(X)[0] @ pca.components_.T\n",
    "    return np.argpartition(predictions, -n_best)[-n_best:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deficient\n",
      "sounds\n",
      "gump\n",
      "rose\n",
      "admitted\n"
     ]
    }
   ],
   "source": [
    "possible = predict_next_word(\"A snail walks into a bar and\", 5)\n",
    "for idx in possible:\n",
    "    print(reverse_tokens_dict[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the next word predictions here are quite bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, n_words, creativity=3):\n",
    "    word_sequence = input_text.split()\n",
    "    current = 0\n",
    "    for _ in range(n_words):\n",
    "        sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
    "        try:\n",
    "            choice = reverse_tokens_dict[random.choice(predict_next_word(sub_sequence, creativity))]\n",
    "        except:\n",
    "            choice = random.choice(unique_tokens)\n",
    "        word_sequence.append(choice)\n",
    "        current += 1\n",
    "    return \" \".join(word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A snail walks into a bar and orders rose 4 suffering buying dinner severed envy admitted sounds gump assistant etc admitted everything assistant'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"A snail walks into a bar and orders\", 15, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is quite bad also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: pre-trained word embedding + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a large word embedding from https://github.com/stanfordnlp/GloVe?tab=readme-ov-file. It has around 1.9 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [02:03, 15516.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_dict = dict()\n",
    "\n",
    "with open(\"glove.42B.300d.txt\", \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        word_and_vec = line.split()\n",
    "        word = word_and_vec[0]\n",
    "        vec = np.array(list(map(float, word_and_vec[1:])))\n",
    "        pretrained_dict[word] = vec\n",
    "\n",
    "print(len(pretrained_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring back all tokens and see how many words we do not know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david', 'carradine', 'farrah', 'fawcett', 'michael', 'jackson', 'billy', 'mays', 'and', 'walter', 'cronkite', 'walk', 'into', 'a', 'bar', 'and', 'die', 'your', 'turn', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', 'what']\n",
      "32093292\n",
      "134157\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|#')\n",
    "tokens = tokenizer.tokenize(text.lower())\n",
    "print(tokens[20:50])\n",
    "print(len(tokens))\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8512\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set(tokens)\n",
    "tokens_dict = {word: i for i, word in enumerate(unique_tokens)}\n",
    "reverse_tokens_dict = {i: word for i, word in enumerate(unique_tokens)}\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_tokens.difference(set(pretrained_dict.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding does not have around 30% of our unique words, but probably they do not appear much so it is not that bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99990/99990 [00:00<00:00, 137418.76it/s]\n",
      "100%|██████████| 99990/99990 [00:02<00:00, 40972.11it/s]\n"
     ]
    }
   ],
   "source": [
    "input_words = []\n",
    "next_words = []\n",
    "for i in tqdm(range(len(tokens) - n_context)):\n",
    "    input_words.append(tokens[i:i+n_context])\n",
    "    next_words.append(tokens[i+n_context])\n",
    "\n",
    "X = np.zeros(shape=(len(input_words), n_context, 300), dtype='float32')\n",
    "y = np.zeros(shape=(len(input_words), 300), dtype='float32')\n",
    "\n",
    "for i, words in enumerate(tqdm(input_words)):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i, j] = pretrained_dict.setdefault(word, np.zeros(300))\n",
    "    y[i] = pretrained_dict.setdefault(next_words[i], np.zeros(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99990, 10, 300) (99990, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(n_context, 300), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 25s 23ms/step - loss: 0.1212\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 0.1003\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 0.0990\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(learning_rate=0.05)\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=optimizer)\n",
    "history = model.fit(X, y, batch_size=100, epochs=3, shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text):\n",
    "    input_text = input_text.lower().split()\n",
    "    X = np.zeros((1, n_context, 300))\n",
    "    for i in range(min(n_context, len(input_text))):\n",
    "        word = input_text[-i-1]\n",
    "        X[0, i] = pretrained_dict.setdefault(word, np.zeros(300))\n",
    "        \n",
    "    predictions = model.predict(X)[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 524ms/step\n",
      "even\n"
     ]
    }
   ],
   "source": [
    "next_word_vec = predict_next_word(\"A snail walks into a bar and says once\")\n",
    "min_dist = -1\n",
    "closest_word = \"\"\n",
    "for word, vec in pretrained_dict.items():\n",
    "    dist = np.linalg.norm(vec - next_word_vec)\n",
    "    if min_dist == -1 or dist < min_dist:\n",
    "        closest_word = word\n",
    "        min_dist = dist\n",
    "\n",
    "print(closest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, n_words):\n",
    "    word_sequence = input_text.split()\n",
    "    current = 0\n",
    "    for _ in tqdm(range(n_words)):\n",
    "        sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
    "        next_word_vec = predict_next_word(input_text)\n",
    "        min_dist = -1\n",
    "        closest_word = \"\"\n",
    "        for word, vec in pretrained_dict.items():\n",
    "            dist = np.linalg.norm(vec - next_word_vec)\n",
    "            if min_dist == -1 or dist < min_dist:\n",
    "                closest_word = word\n",
    "                min_dist = dist\n",
    "\n",
    "        word_sequence.append(closest_word)\n",
    "        current += 1\n",
    "    return \" \".join(word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:25<01:42, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:57<01:27, 29.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 294ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:32<01:03, 31.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 206ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:04<00:32, 32.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:40<00:00, 32.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the chicken cross the road once once once once once'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"Why did the chicken cross the road\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
